python3 -m venv scrapytut

source scrapytut/bin/activate
>> select interpreter to local

# Install scrapy
pip install scrapy

scrapy startproject postscrape
>> cd postscrape
>> create new file in spider >> posts_spider.py

>>>>>>>>>>>>>> scrapy crawl "name_in_spider" <<<<<<<<<<<

RESULT => HTML file

scrapy shell <domain that want to crawl>
>> check on response
e.g. response.css('title')
response.css('title').get()
response.css('title::text').get()

response.css('h3::text').get()
response.css('h3::text')[1].get()
response.css('h3::text')[2].get()

response.css('h3::text').getall()
====================
response.css('.post-header').getall()
response.css('.post-header').get()

response.css('.post-header a::text').get()

===== REGEX =====
response.css('p::text').re(r's\w+') << Start with s
=================
response.xpath('//h3/text()').extract()
response.xpath('//h3/text()').getall()

=================

post = response.css('div.post-item')[0]
title = post.css('.post-header h2 a::text')[0].get()
date = post.css('.post-header a::text')[1].get() 
author = post.css('.post-header a::text')[2].get() 

for post in response.css('div.post-item'):
	title = post.css('.post-header h2 a::text')[0].get()
	date = post.css('.post-header a::text')[1].get() 
	author = post.css('.post-header a::text')[2].get() 
	print(dict(title=title, date=date, author=author))